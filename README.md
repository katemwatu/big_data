# big_data
# Big Data Integration & Dashboard Project
This repository documents a big-data initiative I owned as **Product Owner**, consolidating multi-format data into a central data lake and delivering executive dashboards.  
It highlights my skills in data governance, KPI design, architecture approval, and UX consulting.

## Objective
Provide stakeholders with a unified, high-quality analytics view by integrating large, varied data sources and ensuring a scalable, user-friendly dashboard experience.


## Data Sources
- **JSON files** from internal platforms
- **CSV files** generated internally
- **External API links** providing live metrics
- **Secondary dashboard** (data scraped or exported)
- **SAP system** (transactional data)

All data shown here is anonymised or sample data created to mirror the structure of the original sources.

## Architecture Overview
- **Ingestion**: Automated weekly retrieval of JSON and CSV files via a bot.
- **Data Lake**: Centralised storage of raw and transformed data; supports high-volume, multi-format input.
- **Transformation**: Cleaned, normalised, and joined data into curated layers for analytics.
- **Presentation**: Built dashboards (Power BI) to render KPIs across all sources.

## My Role as Product Owner
- **Data Governance:** Ensured incoming data met accuracy and structural standards; defined initial cleaning rules.
- **Architecture Approval:** Reviewed and approved the underlying data lake architecture and transformation approach proposed by developers.
- **KPI & Metrics Design:** Defined and validated the KPIs, calculations, and performance metrics used in dashboards.
- **Requirements & Prototypes:** Approved dashboard designs and prototypes; provided UX consulting to ensure clarity and usability.
- **Release Management:** Approved release versions and ensured alignment with stakeholder expectations.


## Tools & Technologies
Data Lake (Azure) • Power BI • DAX • Power Query • SAP connectors • API integration • Automation Anywhere bot • JSON/CSV handling

## Development Highlights
- Automated weekly ingestion of JSON and CSV files via a bot, reducing manual work.
- Unified five heterogeneous data sources in a single data lake for scalable analytics.
- Created a curated dataset for executive dashboards with near-real-time updates.


## Outcome
This project demonstrates how, as a **Product Owner**, I guided a big-data pipeline from ingestion to dashboard release — ensuring high-quality data, well-defined KPIs, scalable architecture, and user-centred design.
